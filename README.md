# Machine-Learning---NLP-Skip-Gram-Word-Embeddings
This notebook focuses on implementing word embeddings, compact and dense vector representations of words that encode their semantic meaning. Specifically, we implement the Word2Vec Skip-Gram model and apply negative sampling to optimize training efficiency.
