{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<font>\n",
        "<div dir=ltr align=center>\n",
        "<img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" width=150 height=150> <br>\n",
        "<font color=0F5298 size=7>\n",
        "    Machine learning <br>\n",
        "<font color=2565AE size=5>\n",
        "    Computer Engineering Department <br>\n",
        "    Fall 2024<br>\n",
        "<font color=3C99D size=5>\n",
        "    Practical Assignment 5 - NLP - Skip-Gram <br>\n",
        "<font color=0CBCDF size=4>\n",
        "   &#x1F335; Amirhossein Akbari  &#x1F335;\n",
        "</div>\n",
        "\n",
        "____"
      ],
      "metadata": {
        "id": "0QOmoitc9qC4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "<font color=9999FF size=4>\n",
        "&#x1F388; Full Name :mobina kochaknia\n",
        "<br>\n",
        "<font color=9999FF size=4>\n",
        "&#x1F388; Student Number :401106396"
      ],
      "metadata": {
        "id": "35Y0nSE3-wuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=0080FF size=3>\n",
        "This notebook explores word embeddings, compact and dense vector representations of words that capture their textual meaning. This notebook focusing on implementing the Word2Vec algorithm using the Skip-gram architecture and negative sampling.\n",
        "</font>\n",
        "<br>\n",
        "\n",
        "**Note:**\n",
        "<br>\n",
        "<font color=66B2FF size=2>In this notebook, you are free to use any function or model from TensorFlow to assist with the implementation. However, PyTorch is not permitted for this exercise. This ensures consistency and alignment with the tools being focused on.</font>\n",
        "<br>\n",
        "<font color=red size=3>**Run All Cells Before Submission**</font>: <font color=FF99CC size=2>Before saving and submitting your notebook, please ensure you run all cells from start to finish. This practice guarantees that your notebook is self-consistent and can be evaluated correctly by others.</font>"
      ],
      "metadata": {
        "id": "i4spZpsq_Pxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=#ffb578 size=3>\n",
        "you are free to modify, add, or remove any cells as you see fit to complete your tasks. Feel free to change any of the provided code or content to better suit your understanding and approach to the problems.\n",
        "\n",
        "- **Questions**: If you have any questions or require clarifications as you work through the notebook, please do not hesitate to ask. You can post your queries on Quera or reach out via Telegram.\n",
        "- **Feedback**: We encourage you to seek feedback and engage in discussions to enhance your learning experience and improve your solutions.\n",
        "</font>"
      ],
      "metadata": {
        "id": "hZCaUs-3FsJk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-10T13:12:41.263643Z",
          "iopub.status.busy": "2020-10-10T13:12:41.262979Z",
          "iopub.status.idle": "2020-10-10T13:12:49.471771Z",
          "shell.execute_reply": "2020-10-10T13:12:49.471205Z"
        },
        "id": "hoV5vSSSbIp0",
        "outputId": "3725bc29-fa2d-4384-dea1-67fc56e662b5",
        "papermill": {
          "duration": 8.238801,
          "end_time": "2020-10-10T13:12:49.471884",
          "exception": false,
          "start_time": "2020-10-10T13:12:41.233083",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import io\n",
        "import math\n",
        "import gzip\n",
        "import nltk\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gensim.downloader as api\n",
        "import tensorflow_datasets as tfds\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import skipgrams\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pwuegqx-JWf",
        "papermill": {
          "duration": 0.023154,
          "end_time": "2020-10-10T13:12:49.521385",
          "exception": false,
          "start_time": "2020-10-10T13:12:49.498231",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Downloading Dataset\n",
        "We're going to use text8 dataset. Text8 is first 100,000,000 bytes of plain text from Wikipedia. It's mainly used for testing purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-10T13:12:49.580623Z",
          "iopub.status.busy": "2020-10-10T13:12:49.579765Z",
          "iopub.status.idle": "2020-10-10T13:13:03.778660Z",
          "shell.execute_reply": "2020-10-10T13:13:03.779347Z"
        },
        "id": "XG-FjuVEFLGW",
        "papermill": {
          "duration": 14.234694,
          "end_time": "2020-10-10T13:13:03.779495",
          "exception": false,
          "start_time": "2020-10-10T13:12:49.544801",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "  text8_zip_file_path = api.load('text8', return_path=True)\n",
        "  with gzip.open(text8_zip_file_path, 'rb') as file:\n",
        "    file_content = file.read()\n",
        "  wiki = file_content.decode()\n",
        "  return wiki\n",
        "\n",
        "wiki = load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYBT_-Vy_af4",
        "papermill": {
          "duration": 1.449897,
          "end_time": "2020-10-10T13:13:06.611079",
          "exception": false,
          "start_time": "2020-10-10T13:13:05.161182",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Preprocessing data\n",
        "\n",
        "**Stopwords removal** - Begin by removing stopwords from the dataset, as they provide little to no value in learning word embeddings. Ensure your preprocessing pipeline filters out commonly used words such as \"the,\" \"and,\" or \"of\" that do not contribute to meaningful semantic relationships.\n",
        "\n",
        "---\n",
        "\n",
        "**Subsampling words** - In a large corpora, most frequent words can easily occur hundreds of millions of times and such words usually don't bring much information to the table.  It is of essential importance to cut down on their frequencies to mitigate the negative impact it adds. For example, co-occurrences of \"English\" and \"Spanish\" benefit much more than co-occurrences of \"English\" and \"the\" or \"Spanish\" and \"of\". To counter the imbalance between rare and frequent words Mikolov et. al came up with the following heuristic formula for determining probability to drop a particular word:\n",
        "\n",
        "![formula.png](data:image/png;base64,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)\n",
        "\n",
        "where t is threshold value (heuristically set to 1e-5) and f(w) is frequency of the word.\n",
        "\n",
        "Implement a subsampling mechanism to handle overly frequent words in the corpus. Use the heuristic formula provided by Mikolov et al. to calculate the probability of dropping a word based on its frequency. This step ensures the corpus maintains a balance between rare and frequent words, improving the quality of word co-occurrence relationships.\n",
        "\n",
        "---\n",
        "\n",
        "**Filtering words** - Filter out words that occur only once in the dataset, as they lack sufficient context to be represented effectively. Retain only those words that appear at least five times in the corpus to minimize noise and enhance the overall quality of the embeddings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qzFvHabce2ni"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-10T13:13:09.551725Z",
          "iopub.status.busy": "2020-10-10T13:13:09.441732Z",
          "iopub.status.idle": "2020-10-10T13:13:11.452524Z",
          "shell.execute_reply": "2020-10-10T13:13:11.451989Z"
        },
        "id": "wp50T2OqA-7L",
        "papermill": {
          "duration": 3.4315,
          "end_time": "2020-10-10T13:13:11.452631",
          "exception": false,
          "start_time": "2020-10-10T13:13:08.021131",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocesses the input text by performing standardization, stopword removal,\n",
        "    frequency-based filtering, and subsampling of overly frequent words.\n",
        "    \"\"\"\n",
        "    # Step 1: Replace punctuation with tokens to standardize the text\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = text.replace('\\n', ' ')  # Replace newlines with spaces\n",
        "\n",
        "    # Step 2: Convert text to lowercase and remove unnecessary whitespaces\n",
        "    text = text.lower().strip()\n",
        "\n",
        "    # Step 3: Remove stopwords\n",
        "    stopwords = set([\n",
        "        \"the\", \"and\", \"of\", \"in\", \"to\", \"a\", \"is\", \"that\", \"it\", \"on\", \"for\", \"with\",\n",
        "        \"as\", \"was\", \"at\", \"by\", \"an\", \"be\", \"this\", \"which\", \"or\", \"from\", \"but\",\n",
        "        \"not\", \"are\", \"have\", \"has\", \"had\", \"were\", \"will\", \"would\", \"should\"\n",
        "    ])\n",
        "    tokens = text.split()\n",
        "    filtered_tokens = [word for word in tokens if word not in stopwords]\n",
        "\n",
        "    # Step 4: Remove words with frequency less than 5\n",
        "    word_counts = Counter(filtered_tokens)\n",
        "    filtered_tokens = [word for word in filtered_tokens if word_counts[word] >= 5]\n",
        "\n",
        "    # Step 5: Subsample words using a threshold value\n",
        "    def subsample(word, frequency, total_count, threshold=1e-5):\n",
        "        \"\"\"Determines whether a word should be included based on its frequency.\"\"\"\n",
        "        f_w = frequency / total_count\n",
        "        probability = 1 - np.sqrt(threshold / f_w)\n",
        "        return np.random.rand() > probability\n",
        "\n",
        "    total_word_count = sum(word_counts.values())\n",
        "    final_tokens = [\n",
        "        word for word in filtered_tokens\n",
        "        if subsample(word, word_counts[word], total_word_count)\n",
        "    ]\n",
        "\n",
        "    return final_tokens, word_counts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4260B2ZFJ_EA",
        "papermill": {
          "duration": 1.777391,
          "end_time": "2020-10-10T13:13:14.603396",
          "exception": false,
          "start_time": "2020-10-10T13:13:12.826005",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "It's always a good idea to take a quick look at preprocessed sample before heading further - you might observe few things that if handled can enrich or correct your data. More like a validation step this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-10T13:13:17.665608Z",
          "iopub.status.busy": "2020-10-10T13:13:17.664211Z",
          "iopub.status.idle": "2020-10-10T13:13:17.667915Z",
          "shell.execute_reply": "2020-10-10T13:13:17.668396Z"
        },
        "id": "_oNvdt-v1dw0",
        "papermill": {
          "duration": 1.689149,
          "end_time": "2020-10-10T13:13:17.668521",
          "exception": false,
          "start_time": "2020-10-10T13:13:15.979372",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fae1431f-cdf0-4059-f07f-dbf067a912ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample of preprocessed words (1500 to 1550):\n",
            "['criticisms', 'violence', 'violence', 'too', 'violent', 'engels', 'enough', 'authoritarian', 'whereby', 'imposes', 'bayonets', 'authoritarian', 'party', 'want', 'fought', 'vain', 'terror', 'arms', 'inspire', 'paris', 'commune', 'lasted', 'armed', 'bourgeois', 'utopianism', 'anarchism', 'criticised', 'utopian', 'agree', 'nice', 'carl', 'criticizes', 'unrealistically', 'utopian', 'lesser', 'society', 'without', 'repressive', 'holds', 'intentions', 'cease', 'repressive', 'disappears', 'absurdity', 'however', 'noted', 'anarchists', 'utopian', 'benjamin', 'advocate']\n"
          ]
        }
      ],
      "source": [
        "# Take a quick look at a slice of preprocessed words (e.g., index 1500 to 1550)\n",
        "\n",
        "processed_tokens, word_counts = preprocess_text(wiki)\n",
        "\n",
        "# Inspect a slice of preprocessed tokens\n",
        "sample_start, sample_end = 1500, 1550\n",
        "preprocessed_sample = processed_tokens[sample_start:sample_end]\n",
        "\n",
        "# Display the sample\n",
        "print(f\"Sample of preprocessed words ({sample_start} to {sample_end}):\")\n",
        "print(preprocessed_sample)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCkFtaa_KrTb",
        "papermill": {
          "duration": 1.426874,
          "end_time": "2020-10-10T13:13:20.673211",
          "exception": false,
          "start_time": "2020-10-10T13:13:19.246337",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Hyperparameters\n",
        "Setting a few hyperparamters required for gnerating batches and for deciding the size of word embeddings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-10T13:13:23.483005Z",
          "iopub.status.busy": "2020-10-10T13:13:23.482371Z",
          "iopub.status.idle": "2020-10-10T13:13:23.486821Z",
          "shell.execute_reply": "2020-10-10T13:13:23.486338Z"
        },
        "id": "mJLzBkSIKoMx",
        "papermill": {
          "duration": 1.447402,
          "end_time": "2020-10-10T13:13:23.486929",
          "exception": false,
          "start_time": "2020-10-10T13:13:22.039527",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "WINDOW_SIZE = 2\n",
        "NEGATIVE_SAMPLES = 2\n",
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oO7N0ZsLofI",
        "papermill": {
          "duration": 1.481198,
          "end_time": "2020-10-10T13:13:26.663213",
          "exception": false,
          "start_time": "2020-10-10T13:13:25.182015",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Preparing TensorFlow Dataset using Skipgrams\n",
        "\n",
        "**Generating Skipgrams**\n",
        "\n",
        "Tokenize your preprocessed textual data and convert the words into their corresponding vectorized tokens. Then, use the `skipgrams` function provided by Keras to generate (word, context) pairs. Ensure the following steps are completed:\n",
        "\n",
        "- Generate positive samples: (word, word in the same window), with label 1.  \n",
        "- Generate negative samples: (word, random word from the vocabulary), with label 0.  \n",
        "\n",
        "Refer to Mikolov et al.'s paper, [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781v3.pdf), for more details on Skipgrams.\n",
        "\n",
        "---\n",
        "\n",
        "**Negative Sampling**\n",
        "\n",
        "For each input word, implement the negative sampling approach to optimize the training process. Transform the problem of predicting context words into independent binary classification tasks. For every (target, context) pair, generate random negative (target, ~context) samples. This step will reduce computational complexity and make training more efficient.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-10T13:14:05.592072Z",
          "iopub.status.busy": "2020-10-10T13:14:05.591186Z",
          "iopub.status.idle": "2020-10-10T13:14:12.984172Z",
          "shell.execute_reply": "2020-10-10T13:14:12.984873Z"
        },
        "id": "JRHxw7X4zOpg",
        "papermill": {
          "duration": 8.912486,
          "end_time": "2020-10-10T13:14:12.985042",
          "exception": false,
          "start_time": "2020-10-10T13:14:04.072556",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee4788b1-d0a7-4e0f-9e3a-358e67a5142a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of batches in the training dataset: 21445\n",
            "Number of batches in the testing dataset: 2382\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Hyperparameters\n",
        "WINDOW_SIZE = 2\n",
        "NEGATIVE_SAMPLES = 2\n",
        "BUFFER_SIZE = 10000\n",
        "TARGET_BATCHES_TRAIN = 21445  # Target number of batches for training\n",
        "TARGET_BATCHES_TEST = 2382   # Target number of batches for testing\n",
        "TARGET_SAMPLES_TRAIN = TARGET_BATCHES_TRAIN * 128  # Assuming batch size of 128\n",
        "TARGET_SAMPLES_TEST = TARGET_BATCHES_TEST * 128\n",
        "\n",
        "\n",
        "# Step 1: Initialize and fit the tokenizer on preprocessed words\n",
        "def initialize_tokenizer(preprocessed_words):\n",
        "    \"\"\"Creates and fits a tokenizer on the preprocessed words.\"\"\"\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(preprocessed_words)\n",
        "    return tokenizer\n",
        "\n",
        "# Assuming `processed_tokens` is your preprocessed data\n",
        "tokenizer = initialize_tokenizer(processed_tokens)\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1  # Including padding index\n",
        "\n",
        "# Step 2: Vectorize the words using the tokenizer's word index\n",
        "vectorized_tokens = tokenizer.texts_to_sequences([processed_tokens])[0]\n",
        "\n",
        "# Step 3: Generate skipgram pairs and labels\n",
        "def generate_skipgram_pairs(tokens, vocab_size, window_size, negative_samples):\n",
        "    \"\"\"Generates skipgram pairs with labels.\"\"\"\n",
        "    skipgram_pairs, skipgram_labels = skipgrams(\n",
        "        sequence=tokens,\n",
        "        vocabulary_size=vocab_size,\n",
        "        window_size=window_size,\n",
        "        negative_samples=negative_samples\n",
        "    )\n",
        "    targets = np.array([pair[0] for pair in skipgram_pairs], dtype=np.int32)\n",
        "    contexts = np.array([pair[1] for pair in skipgram_pairs], dtype=np.int32)\n",
        "    labels = np.array(skipgram_labels, dtype=np.int32)\n",
        "    return targets, contexts, labels\n",
        "\n",
        "# Generate skipgram pairs and labels\n",
        "targets, contexts, labels = generate_skipgram_pairs(\n",
        "    tokens=vectorized_tokens,\n",
        "    vocab_size=vocab_size,\n",
        "    window_size=WINDOW_SIZE,\n",
        "    negative_samples=NEGATIVE_SAMPLES\n",
        ")\n",
        "\n",
        "# Step 4: Extract target and context words from the generated pairs\n",
        "# This is done during the `generate_skipgram_pairs` function\n",
        "\n",
        "# Step 5: Split the data into training and testing sets\n",
        "X = np.column_stack((targets, contexts))  # Combine targets and contexts into one array\n",
        "y = labels\n",
        "\n",
        "# Reduce dataset size to target number of samples\n",
        "total_samples = TARGET_SAMPLES_TRAIN + TARGET_SAMPLES_TEST\n",
        "X = X[:total_samples]\n",
        "y = y[:total_samples]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=TARGET_SAMPLES_TEST / total_samples, random_state=42\n",
        ")\n",
        "\n",
        "# Step 6: Create TensorFlow datasets\n",
        "def create_tf_dataset(X, y, batch_size):\n",
        "    \"\"\"Creates a TensorFlow dataset from the input and labels.\"\"\"\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    dataset = dataset.shuffle(BUFFER_SIZE).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "train_dataset = create_tf_dataset(X_train, y_train, BATCH_SIZE)\n",
        "test_dataset = create_tf_dataset(X_test, y_test, BATCH_SIZE)\n",
        "\n",
        "# Count the number of batches\n",
        "num_train_batches = sum(1 for _ in train_dataset)\n",
        "num_test_batches = sum(1 for _ in test_dataset)\n",
        "\n",
        "# Print results\n",
        "print(f\"Number of batches in the training dataset: {num_train_batches}\")\n",
        "print(f\"Number of batches in the testing dataset: {num_test_batches}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUatOx50OXF1",
        "papermill": {
          "duration": 1.350656,
          "end_time": "2020-10-10T13:14:15.692486",
          "exception": false,
          "start_time": "2020-10-10T13:14:14.341830",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Building the Model\n",
        "\n",
        "Use the model subclassing method to build your model. While Sequential and Functional APIs are generally more suitable for most use cases, model subclassing allows you to create the model in an object-oriented way. Follow these steps:\n",
        "\n",
        "1. Define a custom model class by inheriting from `tf.keras.Model`.\n",
        "2. Implement the `__init__` method to define the layers of your model.\n",
        "3. Override the `call` method to define the forward pass of your model.\n",
        "4. Ensure that the model includes embedding layers, a skip-gram architecture, and any other necessary components for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-10T13:14:18.975942Z",
          "iopub.status.busy": "2020-10-10T13:14:18.969008Z",
          "iopub.status.idle": "2020-10-10T13:14:19.276913Z",
          "shell.execute_reply": "2020-10-10T13:14:19.276347Z"
        },
        "id": "6gLxFZ9Eu9Tw",
        "papermill": {
          "duration": 1.935377,
          "end_time": "2020-10-10T13:14:19.277030",
          "exception": false,
          "start_time": "2020-10-10T13:14:17.341653",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5439167e-af23-464c-a5f3-2f7c6cd12fe4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Skip-gram model with data from train_dataset...\n",
            "\n",
            "Training Batch Verification:\n",
            "Center words (first 10): [ 297 3096 2292 2171 4999 4999 1070 4999 4999 4999]\n",
            "Context words (first 10): [4999 4999 4999 4999 4999 4999 4999 4999  419 4999]\n",
            "Output shape: (128, 1)\n",
            "Similarity scores (first 10): [[-0.00067044]\n",
            " [ 0.02168925]\n",
            " [-0.0013479 ]\n",
            " [-0.00993794]\n",
            " [-0.00275239]\n",
            " [-0.00275239]\n",
            " [ 0.00881317]\n",
            " [-0.00275239]\n",
            " [ 0.00780712]\n",
            " [-0.00275239]]\n",
            "\n",
            "Testing Skip-gram model with data from test_dataset...\n",
            "\n",
            "Testing Batch Verification:\n",
            "Center words (first 10): [4152  678 4999 4999  597 4934 4999 4999 2130  455]\n",
            "Context words (first 10): [4999 4999 4999 4999 4999 4999 4999 4999 4999 4999]\n",
            "Output shape: (128, 1)\n",
            "Similarity scores (first 10): [[-0.00107036]\n",
            " [ 0.01282146]\n",
            " [-0.00275239]\n",
            " [-0.00275239]\n",
            " [ 0.01034026]\n",
            " [ 0.00341335]\n",
            " [-0.00275239]\n",
            " [-0.00275239]\n",
            " [-0.00319588]\n",
            " [ 0.0008168 ]]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Step 1: Create a custom model class by subclassing `tf.keras.Model`\n",
        "class SkipGramModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        \"\"\"\n",
        "        Initialize the Skip-gram model with embedding and dense layers.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the vocabulary.\n",
        "            embedding_dim (int): Dimension of the embedding vectors.\n",
        "        \"\"\"\n",
        "        super(SkipGramModel, self).__init__()\n",
        "\n",
        "        # Initialize layers\n",
        "        self.embedding = tf.keras.layers.Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            output_dim=embedding_dim,\n",
        "            name=\"embedding_layer\"\n",
        "        )\n",
        "        self.context_embedding = tf.keras.layers.Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            output_dim=embedding_dim,\n",
        "            name=\"context_embedding_layer\"\n",
        "        )\n",
        "        self.dot_product = tf.keras.layers.Dot(axes=-1, name=\"dot_product_layer\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        Forward pass of the model.\n",
        "\n",
        "        Args:\n",
        "            inputs (tuple): A tuple containing (center_word, context_word).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Dot product of embeddings for center and context words.\n",
        "        \"\"\"\n",
        "        center_word, context_word = inputs  # Unpack inputs\n",
        "        center_emb = self.embedding(center_word)  # Get embeddings for center word\n",
        "        context_emb = self.context_embedding(context_word)  # Get embeddings for context word\n",
        "        similarity = self.dot_product([center_emb, context_emb])  # Compute dot product\n",
        "        return similarity\n",
        "\n",
        "\n",
        "# Function to ensure all word indices are within the valid range\n",
        "def clip_indices(dataset, vocab_size):\n",
        "    \"\"\"Clips the indices in the dataset to be within [0, vocab_size-1].\"\"\"\n",
        "    def clip_fn(x, y):\n",
        "        x_clipped = tf.clip_by_value(x, 0, vocab_size - 1)  # Clip input indices\n",
        "        return x_clipped, y\n",
        "    return dataset.map(clip_fn)\n",
        "\n",
        "\n",
        "# Define hyperparameters\n",
        "vocab_size = 5000  # Adjust based on your dataset\n",
        "embedding_dim = 128  # Size of word embeddings\n",
        "\n",
        "# Assuming train_dataset and test_dataset are already prepared\n",
        "# Clip indices to ensure they are within the valid range\n",
        "train_dataset = clip_indices(train_dataset, vocab_size)\n",
        "test_dataset = clip_indices(test_dataset, vocab_size)\n",
        "\n",
        "# Initialize the Skip-gram model\n",
        "model = SkipGramModel(vocab_size, embedding_dim)\n",
        "\n",
        "# Verify with train_dataset\n",
        "print(\"Testing Skip-gram model with data from train_dataset...\")\n",
        "for (X_batch, y_batch) in train_dataset.take(1):  # Take one batch from train_dataset\n",
        "    center_words = X_batch[:, 0]  # Extract center words from X_batch\n",
        "    context_words = X_batch[:, 1]  # Extract context words from X_batch\n",
        "    output = model((center_words, context_words))  # Forward pass through the model\n",
        "\n",
        "    # Print results for training batch\n",
        "    print(\"\\nTraining Batch Verification:\")\n",
        "    print(\"Center words (first 10):\", center_words.numpy()[:10])\n",
        "    print(\"Context words (first 10):\", context_words.numpy()[:10])\n",
        "    print(\"Output shape:\", output.shape)\n",
        "    print(\"Similarity scores (first 10):\", output.numpy()[:10])\n",
        "    break\n",
        "\n",
        "# Verify with test_dataset\n",
        "print(\"\\nTesting Skip-gram model with data from test_dataset...\")\n",
        "for (X_batch, y_batch) in test_dataset.take(1):  # Take one batch from test_dataset\n",
        "    center_words = X_batch[:, 0]  # Extract center words from X_batch\n",
        "    context_words = X_batch[:, 1]  # Extract context words from X_batch\n",
        "    output = model((center_words, context_words))  # Forward pass through the model\n",
        "\n",
        "    # Print results for testing batch\n",
        "    print(\"\\nTesting Batch Verification:\")\n",
        "    print(\"Center words (first 10):\", center_words.numpy()[:10])\n",
        "    print(\"Context words (first 10):\", context_words.numpy()[:10])\n",
        "    print(\"Output shape:\", output.shape)\n",
        "    print(\"Similarity scores (first 10):\", output.numpy()[:10])\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN3SV3zv0pXG",
        "papermill": {
          "duration": 1.56129,
          "end_time": "2020-10-10T13:14:22.236946",
          "exception": false,
          "start_time": "2020-10-10T13:14:20.675656",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Loss function, Metrics and Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-10T13:14:25.048870Z",
          "iopub.status.busy": "2020-10-10T13:14:25.048241Z",
          "iopub.status.idle": "2020-10-10T13:14:25.068088Z",
          "shell.execute_reply": "2020-10-10T13:14:25.067312Z"
        },
        "id": "ENLrMWOtpixA",
        "papermill": {
          "duration": 1.420264,
          "end_time": "2020-10-10T13:14:25.068193",
          "exception": false,
          "start_time": "2020-10-10T13:14:23.647929",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Define optimizer, loss function, and metrics\n",
        "optimiser = tf.keras.optimizers.Adam(learning_rate=0.001)  # Adam optimizer for efficient training\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)  # Binary cross-entropy for skip-gram architecture\n",
        "train_acc_metric = tf.keras.metrics.BinaryAccuracy(name=\"train_accuracy\")  # Accuracy metric for training\n",
        "val_acc_metric = tf.keras.metrics.BinaryAccuracy(name=\"val_accuracy\")  # Accuracy metric for validation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eyQ_o1EWuJA",
        "papermill": {
          "duration": 1.384257,
          "end_time": "2020-10-10T13:14:27.862984",
          "exception": false,
          "start_time": "2020-10-10T13:14:26.478727",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Training the Model\n",
        "\n",
        "Implement custom training for learning word embeddings to gain finer control over optimization and training tasks. Follow these steps:\n",
        "\n",
        "1. Define a custom training loop that includes forward propagation, loss computation, and backpropagation.\n",
        "2. Use the optimizer of your choice to update the model's weights based on the computed gradients.\n",
        "3. Implement batching for efficient data processing during training.\n",
        "4. Monitor the loss during each epoch to track the model's performance.\n",
        "5. Save the trained embeddings for later use once the training is complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-10T13:14:30.800520Z",
          "iopub.status.busy": "2020-10-10T13:14:30.799563Z",
          "iopub.status.idle": "2020-10-10T13:36:46.204754Z",
          "shell.execute_reply": "2020-10-10T13:36:46.205408Z"
        },
        "id": "oHNb85OL29hu",
        "papermill": {
          "duration": 1336.991023,
          "end_time": "2020-10-10T13:36:46.205587",
          "exception": false,
          "start_time": "2020-10-10T13:14:29.214564",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdba7791-d90f-499a-e515-b94daadbbc4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "Step 0: Train Loss = 0.6917\n",
            "Step 100: Train Loss = 0.6611\n",
            "Step 200: Train Loss = 0.6008\n",
            "Step 300: Train Loss = 0.5749\n",
            "Step 400: Train Loss = 0.5865\n",
            "Step 500: Train Loss = 0.5809\n",
            "Step 600: Train Loss = 0.6055\n",
            "Step 700: Train Loss = 0.5701\n",
            "Step 800: Train Loss = 0.6095\n",
            "Step 900: Train Loss = 0.5405\n",
            "Step 1000: Train Loss = 0.6573\n",
            "Step 1100: Train Loss = 0.5745\n",
            "Step 1200: Train Loss = 0.5970\n",
            "Step 1300: Train Loss = 0.5491\n",
            "Step 1400: Train Loss = 0.5844\n",
            "Step 1500: Train Loss = 0.4916\n",
            "Step 1600: Train Loss = 0.4985\n",
            "Step 1700: Train Loss = 0.5322\n",
            "Step 1800: Train Loss = 0.5583\n",
            "Step 1900: Train Loss = 0.6166\n",
            "Step 2000: Train Loss = 0.5428\n",
            "Step 2100: Train Loss = 0.5308\n",
            "Step 2200: Train Loss = 0.6154\n",
            "Step 2300: Train Loss = 0.6085\n",
            "Step 2400: Train Loss = 0.4640\n",
            "Step 2500: Train Loss = 0.5522\n",
            "Step 2600: Train Loss = 0.5063\n",
            "Step 2700: Train Loss = 0.5722\n",
            "Step 2800: Train Loss = 0.4906\n",
            "Step 2900: Train Loss = 0.5458\n",
            "Step 3000: Train Loss = 0.5408\n",
            "Step 3100: Train Loss = 0.5315\n",
            "Step 3200: Train Loss = 0.5008\n",
            "Step 3300: Train Loss = 0.4973\n",
            "Step 3400: Train Loss = 0.5018\n",
            "Step 3500: Train Loss = 0.5255\n",
            "Step 3600: Train Loss = 0.6028\n",
            "Step 3700: Train Loss = 0.4787\n",
            "Step 3800: Train Loss = 0.5518\n",
            "Step 3900: Train Loss = 0.5480\n",
            "Step 4000: Train Loss = 0.5216\n",
            "Step 4100: Train Loss = 0.5979\n",
            "Step 4200: Train Loss = 0.5687\n",
            "Step 4300: Train Loss = 0.5692\n",
            "Step 4400: Train Loss = 0.5366\n",
            "Step 4500: Train Loss = 0.4873\n",
            "Step 4600: Train Loss = 0.5616\n",
            "Step 4700: Train Loss = 0.5248\n",
            "Step 4800: Train Loss = 0.6775\n",
            "Step 4900: Train Loss = 0.5778\n",
            "Step 5000: Train Loss = 0.6394\n",
            "Step 5100: Train Loss = 0.5981\n",
            "Step 5200: Train Loss = 0.5984\n",
            "Step 5300: Train Loss = 0.5005\n",
            "Step 5400: Train Loss = 0.5862\n",
            "Step 5500: Train Loss = 0.4755\n",
            "Step 5600: Train Loss = 0.5742\n",
            "Step 5700: Train Loss = 0.5619\n",
            "Step 5800: Train Loss = 0.4946\n",
            "Step 5900: Train Loss = 0.5796\n",
            "Step 6000: Train Loss = 0.5639\n",
            "Step 6100: Train Loss = 0.5429\n",
            "Step 6200: Train Loss = 0.5112\n",
            "Step 6300: Train Loss = 0.5823\n",
            "Step 6400: Train Loss = 0.4821\n",
            "Step 6500: Train Loss = 0.5269\n",
            "Step 6600: Train Loss = 0.5715\n",
            "Step 6700: Train Loss = 0.4979\n",
            "Step 6800: Train Loss = 0.4825\n",
            "Step 6900: Train Loss = 0.5309\n",
            "Step 7000: Train Loss = 0.5552\n",
            "Step 7100: Train Loss = 0.5543\n",
            "Step 7200: Train Loss = 0.5609\n",
            "Step 7300: Train Loss = 0.5092\n",
            "Step 7400: Train Loss = 0.5771\n",
            "Step 7500: Train Loss = 0.6295\n",
            "Step 7600: Train Loss = 0.4617\n",
            "Step 7700: Train Loss = 0.5688\n",
            "Step 7800: Train Loss = 0.5639\n",
            "Step 7900: Train Loss = 0.5178\n",
            "Step 8000: Train Loss = 0.4537\n",
            "Step 8100: Train Loss = 0.5424\n",
            "Step 8200: Train Loss = 0.5331\n",
            "Step 8300: Train Loss = 0.5811\n",
            "Step 8400: Train Loss = 0.5614\n",
            "Step 8500: Train Loss = 0.5431\n",
            "Step 8600: Train Loss = 0.5438\n",
            "Step 8700: Train Loss = 0.5549\n",
            "Step 8800: Train Loss = 0.5611\n",
            "Step 8900: Train Loss = 0.5712\n",
            "Step 9000: Train Loss = 0.5286\n",
            "Step 9100: Train Loss = 0.6037\n",
            "Step 9200: Train Loss = 0.5979\n",
            "Step 9300: Train Loss = 0.6996\n",
            "Step 9400: Train Loss = 0.4570\n",
            "Step 9500: Train Loss = 0.5599\n",
            "Step 9600: Train Loss = 0.4958\n",
            "Step 9700: Train Loss = 0.5863\n",
            "Step 9800: Train Loss = 0.6344\n",
            "Step 9900: Train Loss = 0.4991\n",
            "Step 10000: Train Loss = 0.5600\n",
            "Step 10100: Train Loss = 0.5417\n",
            "Step 10200: Train Loss = 0.5499\n",
            "Step 10300: Train Loss = 0.5887\n",
            "Step 10400: Train Loss = 0.4899\n",
            "Step 10500: Train Loss = 0.5990\n",
            "Step 10600: Train Loss = 0.5260\n",
            "Step 10700: Train Loss = 0.5283\n",
            "Step 10800: Train Loss = 0.5399\n",
            "Step 10900: Train Loss = 0.5557\n",
            "Step 11000: Train Loss = 0.6610\n",
            "Step 11100: Train Loss = 0.4768\n",
            "Step 11200: Train Loss = 0.5037\n",
            "Step 11300: Train Loss = 0.4515\n",
            "Step 11400: Train Loss = 0.4963\n",
            "Step 11500: Train Loss = 0.5782\n",
            "Step 11600: Train Loss = 0.5882\n",
            "Step 11700: Train Loss = 0.5632\n",
            "Step 11800: Train Loss = 0.6039\n",
            "Step 11900: Train Loss = 0.5114\n",
            "Step 12000: Train Loss = 0.6149\n",
            "Step 12100: Train Loss = 0.4590\n",
            "Step 12200: Train Loss = 0.4696\n",
            "Step 12300: Train Loss = 0.5604\n",
            "Step 12400: Train Loss = 0.4874\n",
            "Step 12500: Train Loss = 0.4916\n",
            "Step 12600: Train Loss = 0.5904\n",
            "Step 12700: Train Loss = 0.5128\n",
            "Step 12800: Train Loss = 0.5044\n",
            "Step 12900: Train Loss = 0.5319\n",
            "Step 13000: Train Loss = 0.4326\n",
            "Step 13100: Train Loss = 0.5344\n",
            "Step 13200: Train Loss = 0.5332\n",
            "Step 13300: Train Loss = 0.5436\n",
            "Step 13400: Train Loss = 0.5777\n",
            "Step 13500: Train Loss = 0.6332\n",
            "Step 13600: Train Loss = 0.5587\n",
            "Step 13700: Train Loss = 0.4927\n",
            "Step 13800: Train Loss = 0.5025\n",
            "Step 13900: Train Loss = 0.5035\n",
            "Step 14000: Train Loss = 0.5384\n",
            "Step 14100: Train Loss = 0.5194\n",
            "Step 14200: Train Loss = 0.5184\n",
            "Step 14300: Train Loss = 0.4790\n",
            "Step 14400: Train Loss = 0.5602\n",
            "Step 14500: Train Loss = 0.4638\n",
            "Step 14600: Train Loss = 0.4771\n",
            "Step 14700: Train Loss = 0.6066\n",
            "Step 14800: Train Loss = 0.4994\n",
            "Step 14900: Train Loss = 0.5713\n",
            "Step 15000: Train Loss = 0.5411\n",
            "Step 15100: Train Loss = 0.6557\n",
            "Step 15200: Train Loss = 0.5143\n",
            "Step 15300: Train Loss = 0.5751\n",
            "Step 15400: Train Loss = 0.5342\n",
            "Step 15500: Train Loss = 0.5588\n",
            "Step 15600: Train Loss = 0.4774\n",
            "Step 15700: Train Loss = 0.5067\n",
            "Step 15800: Train Loss = 0.4986\n",
            "Step 15900: Train Loss = 0.5074\n",
            "Step 16000: Train Loss = 0.5246\n",
            "Step 16100: Train Loss = 0.4933\n",
            "Step 16200: Train Loss = 0.4701\n",
            "Step 16300: Train Loss = 0.5535\n",
            "Step 16400: Train Loss = 0.5838\n",
            "Step 16500: Train Loss = 0.6085\n",
            "Step 16600: Train Loss = 0.5654\n",
            "Step 16700: Train Loss = 0.4908\n",
            "Step 16800: Train Loss = 0.4417\n",
            "Step 16900: Train Loss = 0.5797\n",
            "Step 17000: Train Loss = 0.5486\n",
            "Step 17100: Train Loss = 0.5502\n",
            "Step 17200: Train Loss = 0.5299\n",
            "Step 17300: Train Loss = 0.6152\n",
            "Step 17400: Train Loss = 0.5529\n",
            "Step 17500: Train Loss = 0.5955\n",
            "Step 17600: Train Loss = 0.5454\n",
            "Step 17700: Train Loss = 0.4722\n",
            "Step 17800: Train Loss = 0.5386\n",
            "Step 17900: Train Loss = 0.5310\n",
            "Step 18000: Train Loss = 0.5116\n",
            "Step 18100: Train Loss = 0.5953\n",
            "Step 18200: Train Loss = 0.5644\n",
            "Step 18300: Train Loss = 0.5364\n",
            "Step 18400: Train Loss = 0.5119\n",
            "Step 18500: Train Loss = 0.5657\n",
            "Step 18600: Train Loss = 0.5820\n",
            "Step 18700: Train Loss = 0.5997\n",
            "Step 18800: Train Loss = 0.5481\n",
            "Step 18900: Train Loss = 0.5521\n",
            "Step 19000: Train Loss = 0.6107\n",
            "Step 19100: Train Loss = 0.5435\n",
            "Step 19200: Train Loss = 0.4932\n",
            "Step 19300: Train Loss = 0.5916\n",
            "Step 19400: Train Loss = 0.5916\n",
            "Step 19500: Train Loss = 0.4884\n",
            "Step 19600: Train Loss = 0.4782\n",
            "Step 19700: Train Loss = 0.4630\n",
            "Step 19800: Train Loss = 0.4537\n",
            "Step 19900: Train Loss = 0.5609\n",
            "Step 20000: Train Loss = 0.4580\n",
            "Step 20100: Train Loss = 0.5729\n",
            "Step 20200: Train Loss = 0.5664\n",
            "Step 20300: Train Loss = 0.5615\n",
            "Step 20400: Train Loss = 0.5142\n",
            "Step 20500: Train Loss = 0.5680\n",
            "Step 20600: Train Loss = 0.5545\n",
            "Step 20700: Train Loss = 0.4897\n",
            "Step 20800: Train Loss = 0.5767\n",
            "Step 20900: Train Loss = 0.5003\n",
            "Step 21000: Train Loss = 0.5414\n",
            "Step 21100: Train Loss = 0.5698\n",
            "Step 21200: Train Loss = 0.5021\n",
            "Step 21300: Train Loss = 0.5592\n",
            "Step 21400: Train Loss = 0.4968\n",
            "Epoch 1 - Train Loss: 0.5448 - Train Accuracy: 0.7547\n",
            "Epoch 1 - Val Loss: 0.5428 - Val Accuracy: 0.7669\n",
            "Time taken for epoch 1: 265.24 seconds\n",
            "\n",
            "Epoch 2/5\n",
            "Step 0: Train Loss = 0.5070\n",
            "Step 100: Train Loss = 0.4642\n",
            "Step 200: Train Loss = 0.5630\n",
            "Step 300: Train Loss = 0.5108\n",
            "Step 400: Train Loss = 0.5865\n",
            "Step 500: Train Loss = 0.5766\n",
            "Step 600: Train Loss = 0.5865\n",
            "Step 700: Train Loss = 0.4855\n",
            "Step 800: Train Loss = 0.5242\n",
            "Step 900: Train Loss = 0.5209\n",
            "Step 1000: Train Loss = 0.5331\n",
            "Step 1100: Train Loss = 0.4633\n",
            "Step 1200: Train Loss = 0.5016\n",
            "Step 1300: Train Loss = 0.5214\n",
            "Step 1400: Train Loss = 0.5966\n",
            "Step 1500: Train Loss = 0.5292\n",
            "Step 1600: Train Loss = 0.5361\n",
            "Step 1700: Train Loss = 0.5677\n",
            "Step 1800: Train Loss = 0.5648\n",
            "Step 1900: Train Loss = 0.5249\n",
            "Step 2000: Train Loss = 0.5179\n",
            "Step 2100: Train Loss = 0.4440\n",
            "Step 2200: Train Loss = 0.5258\n",
            "Step 2300: Train Loss = 0.5986\n",
            "Step 2400: Train Loss = 0.5370\n",
            "Step 2500: Train Loss = 0.5325\n",
            "Step 2600: Train Loss = 0.5497\n",
            "Step 2700: Train Loss = 0.5737\n",
            "Step 2800: Train Loss = 0.5577\n",
            "Step 2900: Train Loss = 0.4822\n",
            "Step 3000: Train Loss = 0.6464\n",
            "Step 3100: Train Loss = 0.5008\n",
            "Step 3200: Train Loss = 0.5548\n",
            "Step 3300: Train Loss = 0.4569\n",
            "Step 3400: Train Loss = 0.4924\n",
            "Step 3500: Train Loss = 0.5668\n",
            "Step 3600: Train Loss = 0.5384\n",
            "Step 3700: Train Loss = 0.4596\n",
            "Step 3800: Train Loss = 0.5370\n",
            "Step 3900: Train Loss = 0.4506\n",
            "Step 4000: Train Loss = 0.6042\n",
            "Step 4100: Train Loss = 0.5246\n",
            "Step 4200: Train Loss = 0.5843\n",
            "Step 4300: Train Loss = 0.5918\n",
            "Step 4400: Train Loss = 0.6391\n",
            "Step 4500: Train Loss = 0.5523\n",
            "Step 4600: Train Loss = 0.5526\n",
            "Step 4700: Train Loss = 0.5842\n",
            "Step 4800: Train Loss = 0.5565\n",
            "Step 4900: Train Loss = 0.5487\n",
            "Step 5000: Train Loss = 0.5536\n",
            "Step 5100: Train Loss = 0.4638\n",
            "Step 5200: Train Loss = 0.4643\n",
            "Step 5300: Train Loss = 0.5802\n",
            "Step 5400: Train Loss = 0.4772\n",
            "Step 5500: Train Loss = 0.5102\n",
            "Step 5600: Train Loss = 0.6019\n",
            "Step 5700: Train Loss = 0.5150\n",
            "Step 5800: Train Loss = 0.5099\n",
            "Step 5900: Train Loss = 0.4977\n",
            "Step 6000: Train Loss = 0.4909\n",
            "Step 6100: Train Loss = 0.5919\n",
            "Step 6200: Train Loss = 0.6232\n",
            "Step 6300: Train Loss = 0.5188\n",
            "Step 6400: Train Loss = 0.5878\n",
            "Step 6500: Train Loss = 0.7233\n",
            "Step 6600: Train Loss = 0.5947\n",
            "Step 6700: Train Loss = 0.5441\n",
            "Step 6800: Train Loss = 0.5867\n",
            "Step 6900: Train Loss = 0.5838\n",
            "Step 7000: Train Loss = 0.5408\n",
            "Step 7100: Train Loss = 0.5382\n",
            "Step 7200: Train Loss = 0.5156\n",
            "Step 7300: Train Loss = 0.5247\n",
            "Step 7400: Train Loss = 0.5608\n",
            "Step 7500: Train Loss = 0.5605\n",
            "Step 7600: Train Loss = 0.6167\n",
            "Step 7700: Train Loss = 0.5431\n",
            "Step 7800: Train Loss = 0.5038\n",
            "Step 7900: Train Loss = 0.5105\n",
            "Step 8000: Train Loss = 0.5636\n",
            "Step 8100: Train Loss = 0.5337\n",
            "Step 8200: Train Loss = 0.5587\n",
            "Step 8300: Train Loss = 0.5607\n",
            "Step 8400: Train Loss = 0.6027\n",
            "Step 8500: Train Loss = 0.5846\n",
            "Step 8600: Train Loss = 0.5109\n",
            "Step 8700: Train Loss = 0.5751\n",
            "Step 8800: Train Loss = 0.6375\n",
            "Step 8900: Train Loss = 0.5065\n",
            "Step 9000: Train Loss = 0.5030\n",
            "Step 9100: Train Loss = 0.5185\n",
            "Step 9200: Train Loss = 0.5738\n",
            "Step 9300: Train Loss = 0.6023\n",
            "Step 9400: Train Loss = 0.5731\n",
            "Step 9500: Train Loss = 0.6685\n",
            "Step 9600: Train Loss = 0.5267\n",
            "Step 9700: Train Loss = 0.5784\n",
            "Step 9800: Train Loss = 0.5591\n",
            "Step 9900: Train Loss = 0.5008\n",
            "Step 10000: Train Loss = 0.5635\n",
            "Step 10100: Train Loss = 0.4972\n",
            "Step 10200: Train Loss = 0.5331\n",
            "Step 10300: Train Loss = 0.4681\n",
            "Step 10400: Train Loss = 0.5047\n",
            "Step 10500: Train Loss = 0.4885\n",
            "Step 10600: Train Loss = 0.6137\n",
            "Step 10700: Train Loss = 0.4428\n",
            "Step 10800: Train Loss = 0.4751\n",
            "Step 10900: Train Loss = 0.4873\n",
            "Step 11000: Train Loss = 0.5637\n",
            "Step 11100: Train Loss = 0.5324\n",
            "Step 11200: Train Loss = 0.6497\n",
            "Step 11300: Train Loss = 0.5459\n",
            "Step 11400: Train Loss = 0.5453\n",
            "Step 11500: Train Loss = 0.5924\n",
            "Step 11600: Train Loss = 0.6045\n",
            "Step 11700: Train Loss = 0.5914\n",
            "Step 11800: Train Loss = 0.5224\n",
            "Step 11900: Train Loss = 0.5590\n",
            "Step 12000: Train Loss = 0.5542\n",
            "Step 12100: Train Loss = 0.5799\n",
            "Step 12200: Train Loss = 0.6164\n",
            "Step 12300: Train Loss = 0.5573\n",
            "Step 12400: Train Loss = 0.5808\n",
            "Step 12500: Train Loss = 0.5404\n",
            "Step 12600: Train Loss = 0.5216\n",
            "Step 12700: Train Loss = 0.5501\n",
            "Step 12800: Train Loss = 0.6125\n",
            "Step 12900: Train Loss = 0.5256\n",
            "Step 13000: Train Loss = 0.5063\n",
            "Step 13100: Train Loss = 0.6140\n",
            "Step 13200: Train Loss = 0.6268\n",
            "Step 13300: Train Loss = 0.5535\n",
            "Step 13400: Train Loss = 0.4802\n",
            "Step 13500: Train Loss = 0.5378\n",
            "Step 13600: Train Loss = 0.5462\n",
            "Step 13700: Train Loss = 0.5700\n",
            "Step 13800: Train Loss = 0.5556\n",
            "Step 13900: Train Loss = 0.5256\n",
            "Step 14000: Train Loss = 0.6078\n",
            "Step 14100: Train Loss = 0.6051\n",
            "Step 14200: Train Loss = 0.4642\n",
            "Step 14300: Train Loss = 0.5191\n",
            "Step 14400: Train Loss = 0.6467\n",
            "Step 14500: Train Loss = 0.5032\n",
            "Step 14600: Train Loss = 0.5308\n",
            "Step 14700: Train Loss = 0.5081\n",
            "Step 14800: Train Loss = 0.4414\n",
            "Step 14900: Train Loss = 0.5741\n",
            "Step 15000: Train Loss = 0.5187\n",
            "Step 15100: Train Loss = 0.5670\n",
            "Step 15200: Train Loss = 0.5163\n",
            "Step 15300: Train Loss = 0.4872\n",
            "Step 15400: Train Loss = 0.5845\n",
            "Step 15500: Train Loss = 0.5151\n",
            "Step 15600: Train Loss = 0.5438\n",
            "Step 15700: Train Loss = 0.5110\n",
            "Step 15800: Train Loss = 0.5005\n",
            "Step 15900: Train Loss = 0.4882\n",
            "Step 16000: Train Loss = 0.5842\n",
            "Step 16100: Train Loss = 0.5910\n",
            "Step 16200: Train Loss = 0.5137\n",
            "Step 16300: Train Loss = 0.5634\n",
            "Step 16400: Train Loss = 0.4993\n",
            "Step 16500: Train Loss = 0.5515\n",
            "Step 16600: Train Loss = 0.5315\n",
            "Step 16700: Train Loss = 0.6364\n",
            "Step 16800: Train Loss = 0.4907\n",
            "Step 16900: Train Loss = 0.4086\n",
            "Step 17000: Train Loss = 0.5618\n",
            "Step 17100: Train Loss = 0.5996\n",
            "Step 17200: Train Loss = 0.5197\n",
            "Step 17300: Train Loss = 0.5371\n",
            "Step 17400: Train Loss = 0.5458\n",
            "Step 17500: Train Loss = 0.5423\n",
            "Step 17600: Train Loss = 0.6257\n",
            "Step 17700: Train Loss = 0.5409\n",
            "Step 17800: Train Loss = 0.5000\n",
            "Step 17900: Train Loss = 0.5299\n",
            "Step 18000: Train Loss = 0.5664\n",
            "Step 18100: Train Loss = 0.5139\n",
            "Step 18200: Train Loss = 0.5232\n",
            "Step 18300: Train Loss = 0.6724\n",
            "Step 18400: Train Loss = 0.5559\n",
            "Step 18500: Train Loss = 0.5400\n",
            "Step 18600: Train Loss = 0.5427\n",
            "Step 18700: Train Loss = 0.4997\n",
            "Step 18800: Train Loss = 0.4833\n",
            "Step 18900: Train Loss = 0.5699\n",
            "Step 19000: Train Loss = 0.5427\n",
            "Step 19100: Train Loss = 0.4548\n",
            "Step 19200: Train Loss = 0.5813\n",
            "Step 19300: Train Loss = 0.4995\n",
            "Step 19400: Train Loss = 0.5882\n",
            "Step 19500: Train Loss = 0.4660\n",
            "Step 19600: Train Loss = 0.4294\n",
            "Step 19700: Train Loss = 0.5253\n",
            "Step 19800: Train Loss = 0.5683\n",
            "Step 19900: Train Loss = 0.5379\n",
            "Step 20000: Train Loss = 0.4740\n",
            "Step 20100: Train Loss = 0.5414\n",
            "Step 20200: Train Loss = 0.5981\n",
            "Step 20300: Train Loss = 0.6377\n",
            "Step 20400: Train Loss = 0.5733\n",
            "Step 20500: Train Loss = 0.4170\n",
            "Step 20600: Train Loss = 0.5977\n",
            "Step 20700: Train Loss = 0.4565\n",
            "Step 20800: Train Loss = 0.4920\n",
            "Step 20900: Train Loss = 0.5371\n",
            "Step 21000: Train Loss = 0.4983\n",
            "Step 21100: Train Loss = 0.5107\n",
            "Step 21200: Train Loss = 0.4989\n",
            "Step 21300: Train Loss = 0.5128\n",
            "Step 21400: Train Loss = 0.5064\n",
            "Epoch 2 - Train Loss: 0.5382 - Train Accuracy: 0.7681\n",
            "Epoch 2 - Val Loss: 0.5445 - Val Accuracy: 0.7651\n",
            "Time taken for epoch 2: 260.08 seconds\n",
            "\n",
            "Epoch 3/5\n",
            "Step 0: Train Loss = 0.4858\n",
            "Step 100: Train Loss = 0.5569\n",
            "Step 200: Train Loss = 0.5907\n",
            "Step 300: Train Loss = 0.4490\n",
            "Step 400: Train Loss = 0.4963\n",
            "Step 500: Train Loss = 0.4767\n",
            "Step 600: Train Loss = 0.5508\n",
            "Step 700: Train Loss = 0.4966\n",
            "Step 800: Train Loss = 0.5659\n",
            "Step 900: Train Loss = 0.6045\n",
            "Step 1000: Train Loss = 0.5681\n",
            "Step 1100: Train Loss = 0.5919\n",
            "Step 1200: Train Loss = 0.5033\n",
            "Step 1300: Train Loss = 0.5322\n",
            "Step 1400: Train Loss = 0.5023\n",
            "Step 1500: Train Loss = 0.5680\n",
            "Step 1600: Train Loss = 0.5222\n",
            "Step 1700: Train Loss = 0.5865\n",
            "Step 1800: Train Loss = 0.5111\n",
            "Step 1900: Train Loss = 0.5037\n",
            "Step 2000: Train Loss = 0.4640\n",
            "Step 2100: Train Loss = 0.5609\n",
            "Step 2200: Train Loss = 0.5895\n",
            "Step 2300: Train Loss = 0.4884\n",
            "Step 2400: Train Loss = 0.5251\n",
            "Step 2500: Train Loss = 0.4730\n",
            "Step 2600: Train Loss = 0.5626\n",
            "Step 2700: Train Loss = 0.5607\n",
            "Step 2800: Train Loss = 0.4908\n",
            "Step 2900: Train Loss = 0.5241\n",
            "Step 3000: Train Loss = 0.5459\n",
            "Step 3100: Train Loss = 0.5705\n",
            "Step 3200: Train Loss = 0.5425\n",
            "Step 3300: Train Loss = 0.5305\n",
            "Step 3400: Train Loss = 0.5438\n",
            "Step 3500: Train Loss = 0.4909\n",
            "Step 3600: Train Loss = 0.5007\n",
            "Step 3700: Train Loss = 0.5261\n",
            "Step 3800: Train Loss = 0.5238\n",
            "Step 3900: Train Loss = 0.5509\n",
            "Step 4000: Train Loss = 0.4980\n",
            "Step 4100: Train Loss = 0.5676\n",
            "Step 4200: Train Loss = 0.6144\n",
            "Step 4300: Train Loss = 0.5748\n",
            "Step 4400: Train Loss = 0.4909\n",
            "Step 4500: Train Loss = 0.5602\n",
            "Step 4600: Train Loss = 0.5507\n",
            "Step 4700: Train Loss = 0.5841\n",
            "Step 4800: Train Loss = 0.5293\n",
            "Step 4900: Train Loss = 0.5560\n",
            "Step 5000: Train Loss = 0.5645\n",
            "Step 5100: Train Loss = 0.5056\n",
            "Step 5200: Train Loss = 0.5026\n",
            "Step 5300: Train Loss = 0.5577\n",
            "Step 5400: Train Loss = 0.4769\n",
            "Step 5500: Train Loss = 0.5101\n",
            "Step 5600: Train Loss = 0.5207\n",
            "Step 5700: Train Loss = 0.5205\n",
            "Step 5800: Train Loss = 0.5274\n",
            "Step 5900: Train Loss = 0.6058\n",
            "Step 6000: Train Loss = 0.5530\n",
            "Step 6100: Train Loss = 0.4684\n",
            "Step 6200: Train Loss = 0.4804\n",
            "Step 6300: Train Loss = 0.5324\n",
            "Step 6400: Train Loss = 0.4951\n",
            "Step 6500: Train Loss = 0.5322\n",
            "Step 6600: Train Loss = 0.5226\n",
            "Step 6700: Train Loss = 0.6114\n",
            "Step 6800: Train Loss = 0.5658\n",
            "Step 6900: Train Loss = 0.5443\n",
            "Step 7000: Train Loss = 0.5768\n",
            "Step 7100: Train Loss = 0.5133\n",
            "Step 7200: Train Loss = 0.5332\n",
            "Step 7300: Train Loss = 0.5744\n",
            "Step 7400: Train Loss = 0.4675\n",
            "Step 7500: Train Loss = 0.5194\n",
            "Step 7600: Train Loss = 0.5119\n",
            "Step 7700: Train Loss = 0.5201\n",
            "Step 7800: Train Loss = 0.5502\n",
            "Step 7900: Train Loss = 0.5107\n",
            "Step 8000: Train Loss = 0.4508\n",
            "Step 8100: Train Loss = 0.5200\n",
            "Step 8200: Train Loss = 0.4680\n",
            "Step 8300: Train Loss = 0.5213\n",
            "Step 8400: Train Loss = 0.4951\n",
            "Step 8500: Train Loss = 0.4640\n",
            "Step 8600: Train Loss = 0.4587\n",
            "Step 8700: Train Loss = 0.5783\n",
            "Step 8800: Train Loss = 0.4313\n",
            "Step 8900: Train Loss = 0.4764\n",
            "Step 9000: Train Loss = 0.5746\n",
            "Step 9100: Train Loss = 0.5760\n",
            "Step 9200: Train Loss = 0.6362\n",
            "Step 9300: Train Loss = 0.5298\n",
            "Step 9400: Train Loss = 0.5468\n",
            "Step 9500: Train Loss = 0.4815\n",
            "Step 9600: Train Loss = 0.4171\n",
            "Step 9700: Train Loss = 0.5627\n",
            "Step 9800: Train Loss = 0.5767\n",
            "Step 9900: Train Loss = 0.4243\n",
            "Step 10000: Train Loss = 0.4970\n",
            "Step 10100: Train Loss = 0.5953\n",
            "Step 10200: Train Loss = 0.4460\n",
            "Step 10300: Train Loss = 0.5078\n",
            "Step 10400: Train Loss = 0.4938\n",
            "Step 10500: Train Loss = 0.5485\n",
            "Step 10600: Train Loss = 0.5884\n",
            "Step 10700: Train Loss = 0.6149\n",
            "Step 10800: Train Loss = 0.4713\n",
            "Step 10900: Train Loss = 0.5095\n",
            "Step 11000: Train Loss = 0.5222\n",
            "Step 11100: Train Loss = 0.6042\n",
            "Step 11200: Train Loss = 0.5182\n",
            "Step 11300: Train Loss = 0.5576\n",
            "Step 11400: Train Loss = 0.4971\n",
            "Step 11500: Train Loss = 0.4539\n",
            "Step 11600: Train Loss = 0.4835\n",
            "Step 11700: Train Loss = 0.4331\n",
            "Step 11800: Train Loss = 0.5404\n",
            "Step 11900: Train Loss = 0.5607\n",
            "Step 12000: Train Loss = 0.5280\n",
            "Step 12100: Train Loss = 0.5216\n",
            "Step 12200: Train Loss = 0.5132\n",
            "Step 12300: Train Loss = 0.5387\n",
            "Step 12400: Train Loss = 0.4933\n",
            "Step 12500: Train Loss = 0.4555\n",
            "Step 12600: Train Loss = 0.4900\n",
            "Step 12700: Train Loss = 0.5573\n",
            "Step 12800: Train Loss = 0.6214\n",
            "Step 12900: Train Loss = 0.6336\n",
            "Step 13000: Train Loss = 0.5289\n",
            "Step 13100: Train Loss = 0.5167\n",
            "Step 13200: Train Loss = 0.5196\n",
            "Step 13300: Train Loss = 0.4972\n",
            "Step 13400: Train Loss = 0.4814\n",
            "Step 13500: Train Loss = 0.5436\n",
            "Step 13600: Train Loss = 0.5794\n",
            "Step 13700: Train Loss = 0.4568\n",
            "Step 13800: Train Loss = 0.5216\n",
            "Step 13900: Train Loss = 0.5440\n",
            "Step 14000: Train Loss = 0.5467\n",
            "Step 14100: Train Loss = 0.4876\n",
            "Step 14200: Train Loss = 0.5420\n",
            "Step 14300: Train Loss = 0.5390\n",
            "Step 14400: Train Loss = 0.6735\n",
            "Step 14500: Train Loss = 0.5582\n",
            "Step 14600: Train Loss = 0.4972\n",
            "Step 14700: Train Loss = 0.5438\n",
            "Step 14800: Train Loss = 0.5464\n",
            "Step 14900: Train Loss = 0.4841\n",
            "Step 15000: Train Loss = 0.4761\n",
            "Step 15100: Train Loss = 0.4772\n",
            "Step 15200: Train Loss = 0.5561\n",
            "Step 15300: Train Loss = 0.5201\n",
            "Step 15400: Train Loss = 0.5556\n",
            "Step 15500: Train Loss = 0.5657\n",
            "Step 15600: Train Loss = 0.5099\n",
            "Step 15700: Train Loss = 0.5670\n",
            "Step 15800: Train Loss = 0.4834\n",
            "Step 15900: Train Loss = 0.5337\n",
            "Step 16000: Train Loss = 0.5932\n",
            "Step 16100: Train Loss = 0.5834\n",
            "Step 16200: Train Loss = 0.5336\n",
            "Step 16300: Train Loss = 0.5388\n",
            "Step 16400: Train Loss = 0.5583\n",
            "Step 16500: Train Loss = 0.5724\n",
            "Step 16600: Train Loss = 0.4965\n",
            "Step 16700: Train Loss = 0.5564\n",
            "Step 16800: Train Loss = 0.5425\n",
            "Step 16900: Train Loss = 0.5421\n",
            "Step 17000: Train Loss = 0.5481\n",
            "Step 17100: Train Loss = 0.5203\n",
            "Step 17200: Train Loss = 0.4800\n",
            "Step 17300: Train Loss = 0.5011\n",
            "Step 17400: Train Loss = 0.4834\n",
            "Step 17500: Train Loss = 0.5863\n",
            "Step 17600: Train Loss = 0.5798\n",
            "Step 17700: Train Loss = 0.4465\n",
            "Step 17800: Train Loss = 0.5231\n",
            "Step 17900: Train Loss = 0.5623\n",
            "Step 18000: Train Loss = 0.4943\n",
            "Step 18100: Train Loss = 0.5085\n",
            "Step 18200: Train Loss = 0.5354\n",
            "Step 18300: Train Loss = 0.5011\n",
            "Step 18400: Train Loss = 0.4552\n",
            "Step 18500: Train Loss = 0.5256\n",
            "Step 18600: Train Loss = 0.5739\n",
            "Step 18700: Train Loss = 0.5038\n",
            "Step 18800: Train Loss = 0.5405\n",
            "Step 18900: Train Loss = 0.4990\n",
            "Step 19000: Train Loss = 0.6281\n",
            "Step 19100: Train Loss = 0.5159\n",
            "Step 19200: Train Loss = 0.5408\n",
            "Step 19300: Train Loss = 0.4872\n",
            "Step 19400: Train Loss = 0.3989\n",
            "Step 19500: Train Loss = 0.6278\n",
            "Step 19600: Train Loss = 0.4223\n",
            "Step 19700: Train Loss = 0.5101\n",
            "Step 19800: Train Loss = 0.5253\n",
            "Step 19900: Train Loss = 0.4978\n",
            "Step 20000: Train Loss = 0.5927\n",
            "Step 20100: Train Loss = 0.4958\n",
            "Step 20200: Train Loss = 0.5884\n",
            "Step 20300: Train Loss = 0.5442\n",
            "Step 20400: Train Loss = 0.4264\n",
            "Step 20500: Train Loss = 0.4672\n",
            "Step 20600: Train Loss = 0.4998\n",
            "Step 20700: Train Loss = 0.5593\n",
            "Step 20800: Train Loss = 0.5144\n",
            "Step 20900: Train Loss = 0.4919\n",
            "Step 21000: Train Loss = 0.5482\n",
            "Step 21100: Train Loss = 0.4495\n",
            "Step 21200: Train Loss = 0.4478\n",
            "Step 21300: Train Loss = 0.5621\n",
            "Step 21400: Train Loss = 0.5522\n",
            "Epoch 3 - Train Loss: 0.5256 - Train Accuracy: 0.7734\n",
            "Epoch 3 - Val Loss: 0.5504 - Val Accuracy: 0.7624\n",
            "Time taken for epoch 3: 255.44 seconds\n",
            "\n",
            "Epoch 4/5\n",
            "Step 0: Train Loss = 0.4964\n",
            "Step 100: Train Loss = 0.5059\n",
            "Step 200: Train Loss = 0.4702\n",
            "Step 300: Train Loss = 0.6525\n",
            "Step 400: Train Loss = 0.5380\n",
            "Step 500: Train Loss = 0.5300\n",
            "Step 600: Train Loss = 0.4392\n",
            "Step 700: Train Loss = 0.5279\n",
            "Step 800: Train Loss = 0.4645\n",
            "Step 900: Train Loss = 0.4525\n",
            "Step 1000: Train Loss = 0.5405\n",
            "Step 1100: Train Loss = 0.5115\n",
            "Step 1200: Train Loss = 0.5125\n",
            "Step 1300: Train Loss = 0.4241\n",
            "Step 1400: Train Loss = 0.4315\n",
            "Step 1500: Train Loss = 0.5356\n",
            "Step 1600: Train Loss = 0.4756\n",
            "Step 1700: Train Loss = 0.4641\n",
            "Step 1800: Train Loss = 0.5414\n",
            "Step 1900: Train Loss = 0.5508\n",
            "Step 2000: Train Loss = 0.4744\n",
            "Step 2100: Train Loss = 0.5365\n",
            "Step 2200: Train Loss = 0.5412\n",
            "Step 2300: Train Loss = 0.4486\n",
            "Step 2400: Train Loss = 0.5074\n",
            "Step 2500: Train Loss = 0.5256\n",
            "Step 2600: Train Loss = 0.5266\n",
            "Step 2700: Train Loss = 0.6316\n",
            "Step 2800: Train Loss = 0.4720\n",
            "Step 2900: Train Loss = 0.4921\n",
            "Step 3000: Train Loss = 0.5427\n",
            "Step 3100: Train Loss = 0.4790\n",
            "Step 3200: Train Loss = 0.5057\n",
            "Step 3300: Train Loss = 0.5845\n",
            "Step 3400: Train Loss = 0.5308\n",
            "Step 3500: Train Loss = 0.5506\n",
            "Step 3600: Train Loss = 0.5453\n",
            "Step 3700: Train Loss = 0.5082\n",
            "Step 3800: Train Loss = 0.4637\n",
            "Step 3900: Train Loss = 0.5613\n",
            "Step 4000: Train Loss = 0.4803\n",
            "Step 4100: Train Loss = 0.5045\n",
            "Step 4200: Train Loss = 0.5482\n",
            "Step 4300: Train Loss = 0.4968\n",
            "Step 4400: Train Loss = 0.4488\n",
            "Step 4500: Train Loss = 0.4889\n",
            "Step 4600: Train Loss = 0.4665\n",
            "Step 4700: Train Loss = 0.4999\n",
            "Step 4800: Train Loss = 0.5221\n",
            "Step 4900: Train Loss = 0.5605\n",
            "Step 5000: Train Loss = 0.4973\n",
            "Step 5100: Train Loss = 0.4831\n",
            "Step 5200: Train Loss = 0.5353\n",
            "Step 5300: Train Loss = 0.5594\n",
            "Step 5400: Train Loss = 0.6135\n",
            "Step 5500: Train Loss = 0.5493\n",
            "Step 5600: Train Loss = 0.5213\n",
            "Step 5700: Train Loss = 0.5129\n",
            "Step 5800: Train Loss = 0.4723\n",
            "Step 5900: Train Loss = 0.4639\n",
            "Step 6000: Train Loss = 0.4797\n",
            "Step 6100: Train Loss = 0.4695\n",
            "Step 6200: Train Loss = 0.5193\n",
            "Step 6300: Train Loss = 0.5514\n",
            "Step 6400: Train Loss = 0.5482\n",
            "Step 6500: Train Loss = 0.5394\n",
            "Step 6600: Train Loss = 0.5936\n",
            "Step 6700: Train Loss = 0.5272\n",
            "Step 6800: Train Loss = 0.5543\n",
            "Step 6900: Train Loss = 0.4606\n",
            "Step 7000: Train Loss = 0.6011\n",
            "Step 7100: Train Loss = 0.6062\n",
            "Step 7200: Train Loss = 0.4137\n",
            "Step 7300: Train Loss = 0.5546\n",
            "Step 7400: Train Loss = 0.4994\n",
            "Step 7500: Train Loss = 0.4566\n",
            "Step 7600: Train Loss = 0.5073\n",
            "Step 7700: Train Loss = 0.5354\n",
            "Step 7800: Train Loss = 0.5346\n",
            "Step 7900: Train Loss = 0.5162\n",
            "Step 8000: Train Loss = 0.5085\n",
            "Step 8100: Train Loss = 0.5285\n",
            "Step 8200: Train Loss = 0.6032\n",
            "Step 8300: Train Loss = 0.5565\n",
            "Step 8400: Train Loss = 0.4827\n",
            "Step 8500: Train Loss = 0.4990\n",
            "Step 8600: Train Loss = 0.5431\n",
            "Step 8700: Train Loss = 0.5341\n",
            "Step 8800: Train Loss = 0.5414\n",
            "Step 8900: Train Loss = 0.5801\n",
            "Step 9000: Train Loss = 0.5841\n",
            "Step 9100: Train Loss = 0.5178\n",
            "Step 9200: Train Loss = 0.5442\n",
            "Step 9300: Train Loss = 0.4894\n",
            "Step 9400: Train Loss = 0.4900\n",
            "Step 9500: Train Loss = 0.4309\n",
            "Step 9600: Train Loss = 0.4393\n",
            "Step 9700: Train Loss = 0.4516\n",
            "Step 9800: Train Loss = 0.5341\n",
            "Step 9900: Train Loss = 0.4457\n",
            "Step 10000: Train Loss = 0.5386\n",
            "Step 10100: Train Loss = 0.5795\n",
            "Step 10200: Train Loss = 0.5118\n",
            "Step 10300: Train Loss = 0.5389\n",
            "Step 10400: Train Loss = 0.5370\n",
            "Step 10500: Train Loss = 0.4652\n",
            "Step 10600: Train Loss = 0.5106\n",
            "Step 10700: Train Loss = 0.5378\n",
            "Step 10800: Train Loss = 0.5606\n",
            "Step 10900: Train Loss = 0.4340\n",
            "Step 11000: Train Loss = 0.6160\n",
            "Step 11100: Train Loss = 0.5364\n",
            "Step 11200: Train Loss = 0.5504\n",
            "Step 11300: Train Loss = 0.5379\n",
            "Step 11400: Train Loss = 0.5138\n",
            "Step 11500: Train Loss = 0.5736\n",
            "Step 11600: Train Loss = 0.4815\n",
            "Step 11700: Train Loss = 0.5868\n",
            "Step 11800: Train Loss = 0.5270\n",
            "Step 11900: Train Loss = 0.4837\n",
            "Step 12000: Train Loss = 0.5785\n",
            "Step 12100: Train Loss = 0.4226\n",
            "Step 12200: Train Loss = 0.4474\n",
            "Step 12300: Train Loss = 0.5002\n",
            "Step 12400: Train Loss = 0.5409\n",
            "Step 12500: Train Loss = 0.5578\n",
            "Step 12600: Train Loss = 0.4526\n",
            "Step 12700: Train Loss = 0.5152\n",
            "Step 12800: Train Loss = 0.4717\n",
            "Step 12900: Train Loss = 0.5167\n",
            "Step 13000: Train Loss = 0.5247\n",
            "Step 13100: Train Loss = 0.6049\n",
            "Step 13200: Train Loss = 0.5696\n",
            "Step 13300: Train Loss = 0.5235\n",
            "Step 13400: Train Loss = 0.4932\n",
            "Step 13500: Train Loss = 0.4863\n",
            "Step 13600: Train Loss = 0.5570\n",
            "Step 13700: Train Loss = 0.4184\n",
            "Step 13800: Train Loss = 0.5191\n",
            "Step 13900: Train Loss = 0.5389\n",
            "Step 14000: Train Loss = 0.6054\n",
            "Step 14100: Train Loss = 0.5655\n",
            "Step 14200: Train Loss = 0.4925\n",
            "Step 14300: Train Loss = 0.5261\n",
            "Step 14400: Train Loss = 0.5310\n",
            "Step 14500: Train Loss = 0.4507\n",
            "Step 14600: Train Loss = 0.5416\n",
            "Step 14700: Train Loss = 0.4719\n",
            "Step 14800: Train Loss = 0.5043\n",
            "Step 14900: Train Loss = 0.5143\n",
            "Step 15000: Train Loss = 0.4886\n",
            "Step 15100: Train Loss = 0.5110\n",
            "Step 15200: Train Loss = 0.4731\n",
            "Step 15300: Train Loss = 0.5291\n",
            "Step 15400: Train Loss = 0.5400\n",
            "Step 15500: Train Loss = 0.4659\n",
            "Step 15600: Train Loss = 0.5432\n",
            "Step 15700: Train Loss = 0.5484\n",
            "Step 15800: Train Loss = 0.5343\n",
            "Step 15900: Train Loss = 0.5440\n",
            "Step 16000: Train Loss = 0.5243\n",
            "Step 16100: Train Loss = 0.5135\n",
            "Step 16200: Train Loss = 0.5805\n",
            "Step 16300: Train Loss = 0.5535\n",
            "Step 16400: Train Loss = 0.4841\n",
            "Step 16500: Train Loss = 0.4588\n",
            "Step 16600: Train Loss = 0.4826\n",
            "Step 16700: Train Loss = 0.4780\n",
            "Step 16800: Train Loss = 0.4976\n",
            "Step 16900: Train Loss = 0.5431\n",
            "Step 17000: Train Loss = 0.6281\n",
            "Step 17100: Train Loss = 0.4724\n",
            "Step 17200: Train Loss = 0.4946\n",
            "Step 17300: Train Loss = 0.4494\n",
            "Step 17400: Train Loss = 0.4649\n",
            "Step 17500: Train Loss = 0.5380\n",
            "Step 17600: Train Loss = 0.4919\n",
            "Step 17700: Train Loss = 0.4710\n",
            "Step 17800: Train Loss = 0.4443\n",
            "Step 17900: Train Loss = 0.5551\n",
            "Step 18000: Train Loss = 0.4521\n",
            "Step 18100: Train Loss = 0.5560\n",
            "Step 18200: Train Loss = 0.5552\n",
            "Step 18300: Train Loss = 0.5071\n",
            "Step 18400: Train Loss = 0.5390\n",
            "Step 18500: Train Loss = 0.5191\n",
            "Step 18600: Train Loss = 0.4658\n",
            "Step 18700: Train Loss = 0.5527\n",
            "Step 18800: Train Loss = 0.4656\n",
            "Step 18900: Train Loss = 0.4794\n",
            "Step 19000: Train Loss = 0.5201\n",
            "Step 19100: Train Loss = 0.5103\n",
            "Step 19200: Train Loss = 0.5571\n",
            "Step 19300: Train Loss = 0.4847\n",
            "Step 19400: Train Loss = 0.5900\n",
            "Step 19500: Train Loss = 0.5121\n",
            "Step 19600: Train Loss = 0.5073\n",
            "Step 19700: Train Loss = 0.5702\n",
            "Step 19800: Train Loss = 0.5642\n",
            "Step 19900: Train Loss = 0.4628\n",
            "Step 20000: Train Loss = 0.5076\n",
            "Step 20100: Train Loss = 0.5139\n",
            "Step 20200: Train Loss = 0.5186\n",
            "Step 20300: Train Loss = 0.4870\n",
            "Step 20400: Train Loss = 0.4764\n",
            "Step 20500: Train Loss = 0.4592\n",
            "Step 20600: Train Loss = 0.5478\n",
            "Step 20700: Train Loss = 0.4720\n",
            "Step 20800: Train Loss = 0.4631\n",
            "Step 20900: Train Loss = 0.5359\n",
            "Step 21000: Train Loss = 0.5081\n",
            "Step 21100: Train Loss = 0.5039\n",
            "Step 21200: Train Loss = 0.5439\n",
            "Step 21300: Train Loss = 0.4578\n",
            "Step 21400: Train Loss = 0.4550\n",
            "Epoch 4 - Train Loss: 0.5083 - Train Accuracy: 0.7813\n",
            "Epoch 4 - Val Loss: 0.5586 - Val Accuracy: 0.7605\n",
            "Time taken for epoch 4: 260.72 seconds\n",
            "\n",
            "Epoch 5/5\n",
            "Step 0: Train Loss = 0.5555\n",
            "Step 100: Train Loss = 0.5224\n",
            "Step 200: Train Loss = 0.4173\n",
            "Step 300: Train Loss = 0.5752\n",
            "Step 400: Train Loss = 0.5445\n",
            "Step 500: Train Loss = 0.4661\n",
            "Step 600: Train Loss = 0.4945\n",
            "Step 700: Train Loss = 0.5106\n",
            "Step 800: Train Loss = 0.4339\n",
            "Step 900: Train Loss = 0.4106\n",
            "Step 1000: Train Loss = 0.4305\n",
            "Step 1100: Train Loss = 0.5485\n",
            "Step 1200: Train Loss = 0.5651\n",
            "Step 1300: Train Loss = 0.5488\n",
            "Step 1400: Train Loss = 0.4721\n",
            "Step 1500: Train Loss = 0.4888\n",
            "Step 1600: Train Loss = 0.5191\n",
            "Step 1700: Train Loss = 0.4350\n",
            "Step 1800: Train Loss = 0.4647\n",
            "Step 1900: Train Loss = 0.5297\n",
            "Step 2000: Train Loss = 0.4791\n",
            "Step 2100: Train Loss = 0.4815\n",
            "Step 2200: Train Loss = 0.5115\n",
            "Step 2300: Train Loss = 0.3888\n",
            "Step 2400: Train Loss = 0.4474\n",
            "Step 2500: Train Loss = 0.3993\n",
            "Step 2600: Train Loss = 0.4639\n",
            "Step 2700: Train Loss = 0.5493\n",
            "Step 2800: Train Loss = 0.4734\n",
            "Step 2900: Train Loss = 0.4529\n",
            "Step 3000: Train Loss = 0.4461\n",
            "Step 3100: Train Loss = 0.5639\n",
            "Step 3200: Train Loss = 0.4760\n",
            "Step 3300: Train Loss = 0.4960\n",
            "Step 3400: Train Loss = 0.5115\n",
            "Step 3500: Train Loss = 0.5469\n",
            "Step 3600: Train Loss = 0.5537\n",
            "Step 3700: Train Loss = 0.4593\n",
            "Step 3800: Train Loss = 0.4448\n",
            "Step 3900: Train Loss = 0.5273\n",
            "Step 4000: Train Loss = 0.4757\n",
            "Step 4100: Train Loss = 0.4744\n",
            "Step 4200: Train Loss = 0.4203\n",
            "Step 4300: Train Loss = 0.5457\n",
            "Step 4400: Train Loss = 0.5127\n",
            "Step 4500: Train Loss = 0.4877\n",
            "Step 4600: Train Loss = 0.5052\n",
            "Step 4700: Train Loss = 0.4357\n",
            "Step 4800: Train Loss = 0.4659\n",
            "Step 4900: Train Loss = 0.4734\n",
            "Step 5000: Train Loss = 0.4406\n",
            "Step 5100: Train Loss = 0.4889\n",
            "Step 5200: Train Loss = 0.4741\n",
            "Step 5300: Train Loss = 0.5150\n",
            "Step 5400: Train Loss = 0.4754\n",
            "Step 5500: Train Loss = 0.4796\n",
            "Step 5600: Train Loss = 0.4980\n",
            "Step 5700: Train Loss = 0.5304\n",
            "Step 5800: Train Loss = 0.5195\n",
            "Step 5900: Train Loss = 0.5030\n",
            "Step 6000: Train Loss = 0.5138\n",
            "Step 6100: Train Loss = 0.4526\n",
            "Step 6200: Train Loss = 0.4708\n",
            "Step 6300: Train Loss = 0.4944\n",
            "Step 6400: Train Loss = 0.4644\n",
            "Step 6500: Train Loss = 0.4751\n",
            "Step 6600: Train Loss = 0.5181\n",
            "Step 6700: Train Loss = 0.4184\n",
            "Step 6800: Train Loss = 0.4800\n",
            "Step 6900: Train Loss = 0.4823\n",
            "Step 7000: Train Loss = 0.4788\n",
            "Step 7100: Train Loss = 0.4873\n",
            "Step 7200: Train Loss = 0.4681\n",
            "Step 7300: Train Loss = 0.4399\n",
            "Step 7400: Train Loss = 0.5154\n",
            "Step 7500: Train Loss = 0.4985\n",
            "Step 7600: Train Loss = 0.5005\n",
            "Step 7700: Train Loss = 0.4536\n",
            "Step 7800: Train Loss = 0.5235\n",
            "Step 7900: Train Loss = 0.4465\n",
            "Step 8000: Train Loss = 0.5004\n",
            "Step 8100: Train Loss = 0.5242\n",
            "Step 8200: Train Loss = 0.5073\n",
            "Step 8300: Train Loss = 0.4602\n",
            "Step 8400: Train Loss = 0.3830\n",
            "Step 8500: Train Loss = 0.5256\n",
            "Step 8600: Train Loss = 0.5566\n",
            "Step 8700: Train Loss = 0.4072\n",
            "Step 8800: Train Loss = 0.4928\n",
            "Step 8900: Train Loss = 0.4846\n",
            "Step 9000: Train Loss = 0.4867\n",
            "Step 9100: Train Loss = 0.5192\n",
            "Step 9200: Train Loss = 0.5413\n",
            "Step 9300: Train Loss = 0.4689\n",
            "Step 9400: Train Loss = 0.4635\n",
            "Step 9500: Train Loss = 0.4299\n",
            "Step 9600: Train Loss = 0.4645\n",
            "Step 9700: Train Loss = 0.6093\n",
            "Step 9800: Train Loss = 0.4249\n",
            "Step 9900: Train Loss = 0.5499\n",
            "Step 10000: Train Loss = 0.4966\n",
            "Step 10100: Train Loss = 0.4280\n",
            "Step 10200: Train Loss = 0.4720\n",
            "Step 10300: Train Loss = 0.4254\n",
            "Step 10400: Train Loss = 0.4104\n",
            "Step 10500: Train Loss = 0.4221\n",
            "Step 10600: Train Loss = 0.4950\n",
            "Step 10700: Train Loss = 0.5498\n",
            "Step 10800: Train Loss = 0.5327\n",
            "Step 10900: Train Loss = 0.4297\n",
            "Step 11000: Train Loss = 0.5799\n",
            "Step 11100: Train Loss = 0.6150\n",
            "Step 11200: Train Loss = 0.5175\n",
            "Step 11300: Train Loss = 0.5774\n",
            "Step 11400: Train Loss = 0.4450\n",
            "Step 11500: Train Loss = 0.5291\n",
            "Step 11600: Train Loss = 0.5248\n",
            "Step 11700: Train Loss = 0.5032\n",
            "Step 11800: Train Loss = 0.4232\n",
            "Step 11900: Train Loss = 0.5292\n",
            "Step 12000: Train Loss = 0.5314\n",
            "Step 12100: Train Loss = 0.4547\n",
            "Step 12200: Train Loss = 0.3789\n",
            "Step 12300: Train Loss = 0.4627\n",
            "Step 12400: Train Loss = 0.4948\n",
            "Step 12500: Train Loss = 0.4866\n",
            "Step 12600: Train Loss = 0.4995\n",
            "Step 12700: Train Loss = 0.5133\n",
            "Step 12800: Train Loss = 0.4134\n",
            "Step 12900: Train Loss = 0.4314\n",
            "Step 13000: Train Loss = 0.5172\n",
            "Step 13100: Train Loss = 0.4619\n",
            "Step 13200: Train Loss = 0.5626\n",
            "Step 13300: Train Loss = 0.4758\n",
            "Step 13400: Train Loss = 0.4174\n",
            "Step 13500: Train Loss = 0.5186\n",
            "Step 13600: Train Loss = 0.5045\n",
            "Step 13700: Train Loss = 0.4556\n",
            "Step 13800: Train Loss = 0.5066\n",
            "Step 13900: Train Loss = 0.4841\n",
            "Step 14000: Train Loss = 0.5001\n",
            "Step 14100: Train Loss = 0.4404\n",
            "Step 14200: Train Loss = 0.4340\n",
            "Step 14300: Train Loss = 0.3560\n",
            "Step 14400: Train Loss = 0.4817\n",
            "Step 14500: Train Loss = 0.4325\n",
            "Step 14600: Train Loss = 0.4151\n",
            "Step 14700: Train Loss = 0.4849\n",
            "Step 14800: Train Loss = 0.6220\n",
            "Step 14900: Train Loss = 0.5486\n",
            "Step 15000: Train Loss = 0.5374\n",
            "Step 15100: Train Loss = 0.5257\n",
            "Step 15200: Train Loss = 0.4672\n",
            "Step 15300: Train Loss = 0.5329\n",
            "Step 15400: Train Loss = 0.5366\n",
            "Step 15500: Train Loss = 0.4807\n",
            "Step 15600: Train Loss = 0.5302\n",
            "Step 15700: Train Loss = 0.5529\n",
            "Step 15800: Train Loss = 0.5264\n",
            "Step 15900: Train Loss = 0.5709\n",
            "Step 16000: Train Loss = 0.5768\n",
            "Step 16100: Train Loss = 0.4894\n",
            "Step 16200: Train Loss = 0.4217\n",
            "Step 16300: Train Loss = 0.4175\n",
            "Step 16400: Train Loss = 0.4355\n",
            "Step 16500: Train Loss = 0.4581\n",
            "Step 16600: Train Loss = 0.5402\n",
            "Step 16700: Train Loss = 0.4664\n",
            "Step 16800: Train Loss = 0.4444\n",
            "Step 16900: Train Loss = 0.4165\n",
            "Step 17000: Train Loss = 0.4850\n",
            "Step 17100: Train Loss = 0.5151\n",
            "Step 17200: Train Loss = 0.4258\n",
            "Step 17300: Train Loss = 0.5140\n",
            "Step 17400: Train Loss = 0.4798\n",
            "Step 17500: Train Loss = 0.6177\n",
            "Step 17600: Train Loss = 0.5775\n",
            "Step 17700: Train Loss = 0.4866\n",
            "Step 17800: Train Loss = 0.5198\n",
            "Step 17900: Train Loss = 0.4585\n",
            "Step 18000: Train Loss = 0.5243\n",
            "Step 18100: Train Loss = 0.5439\n",
            "Step 18200: Train Loss = 0.4939\n",
            "Step 18300: Train Loss = 0.4322\n",
            "Step 18400: Train Loss = 0.5454\n",
            "Step 18500: Train Loss = 0.4668\n",
            "Step 18600: Train Loss = 0.5170\n",
            "Step 18700: Train Loss = 0.4877\n",
            "Step 18800: Train Loss = 0.5076\n",
            "Step 18900: Train Loss = 0.3940\n",
            "Step 19000: Train Loss = 0.4976\n",
            "Step 19100: Train Loss = 0.4613\n",
            "Step 19200: Train Loss = 0.5203\n",
            "Step 19300: Train Loss = 0.4984\n",
            "Step 19400: Train Loss = 0.4471\n",
            "Step 19500: Train Loss = 0.3853\n",
            "Step 19600: Train Loss = 0.4954\n",
            "Step 19700: Train Loss = 0.4698\n",
            "Step 19800: Train Loss = 0.4421\n",
            "Step 19900: Train Loss = 0.4489\n",
            "Step 20000: Train Loss = 0.5108\n",
            "Step 20100: Train Loss = 0.4275\n",
            "Step 20200: Train Loss = 0.5401\n",
            "Step 20300: Train Loss = 0.5321\n",
            "Step 20400: Train Loss = 0.5503\n",
            "Step 20500: Train Loss = 0.4798\n",
            "Step 20600: Train Loss = 0.5172\n",
            "Step 20700: Train Loss = 0.4197\n",
            "Step 20800: Train Loss = 0.5213\n",
            "Step 20900: Train Loss = 0.4925\n",
            "Step 21000: Train Loss = 0.5627\n",
            "Step 21100: Train Loss = 0.5514\n",
            "Step 21200: Train Loss = 0.5167\n",
            "Step 21300: Train Loss = 0.5212\n",
            "Step 21400: Train Loss = 0.4862\n",
            "Epoch 5 - Train Loss: 0.4929 - Train Accuracy: 0.7875\n",
            "Epoch 5 - Val Loss: 0.5703 - Val Accuracy: 0.7593\n",
            "Time taken for epoch 5: 260.61 seconds\n",
            "Trained embeddings saved.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Define hyperparameters\n",
        "vocab_size = 5000\n",
        "embedding_dim = 128\n",
        "epochs = 5\n",
        "batch_size = 128\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Step 1: Define the training step\n",
        "@tf.function\n",
        "def train_step(center_words, context_words, labels, model, optimizer, loss_fn, train_acc_metric):\n",
        "    \"\"\"\n",
        "    Perform a single training step.\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model((center_words, context_words))\n",
        "        loss = loss_fn(labels, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "    train_acc_metric.update_state(labels, predictions)\n",
        "    return loss\n",
        "\n",
        "# Step 2: Define the testing step\n",
        "@tf.function\n",
        "def test_step(center_words, context_words, labels, model, loss_fn, val_acc_metric):\n",
        "    \"\"\"\n",
        "    Perform a single validation step.\n",
        "    \"\"\"\n",
        "    predictions = model((center_words, context_words))\n",
        "    loss = loss_fn(labels, predictions)\n",
        "    val_acc_metric.update_state(labels, predictions)\n",
        "    return loss\n",
        "\n",
        "# Initialize the model, optimizer, loss function, and metrics\n",
        "model = SkipGramModel(vocab_size, embedding_dim)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "train_acc_metric = tf.keras.metrics.BinaryAccuracy(name=\"train_accuracy\")\n",
        "val_acc_metric = tf.keras.metrics.BinaryAccuracy(name=\"val_accuracy\")\n",
        "\n",
        "# Step 3: Initialize the training loop\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    epoch_start_time = time.time()\n",
        "    train_acc_metric.reset_state()\n",
        "    val_acc_metric.reset_state()\n",
        "    total_train_loss = 0.0\n",
        "    total_val_loss = 0.0\n",
        "\n",
        "    # Step 4: Perform training on each batch\n",
        "    for step, (X_batch, y_batch) in enumerate(train_dataset):\n",
        "        center_words = X_batch[:, 0]\n",
        "        context_words = X_batch[:, 1]\n",
        "        loss = train_step(center_words, context_words, y_batch, model, optimizer, loss_fn, train_acc_metric)\n",
        "        total_train_loss += loss.numpy()\n",
        "\n",
        "        # Log training progress every 100 steps\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}: Train Loss = {loss.numpy():.4f}\")\n",
        "\n",
        "    # Step 5: Compute and display training accuracy\n",
        "    train_accuracy = train_acc_metric.result().numpy()\n",
        "    avg_train_loss = total_train_loss / (step + 1)\n",
        "    print(f\"Epoch {epoch + 1} - Train Loss: {avg_train_loss:.4f} - Train Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "    # Step 6: Perform validation on the test dataset\n",
        "    for step, (X_batch, y_batch) in enumerate(test_dataset):\n",
        "        center_words = X_batch[:, 0]\n",
        "        context_words = X_batch[:, 1]\n",
        "        val_loss = test_step(center_words, context_words, y_batch, model, loss_fn, val_acc_metric)\n",
        "        total_val_loss += val_loss.numpy()\n",
        "\n",
        "    # Step 7: Log validation metrics\n",
        "    val_accuracy = val_acc_metric.result().numpy()\n",
        "    avg_val_loss = total_val_loss / (step + 1)\n",
        "    print(f\"Epoch {epoch + 1} - Val Loss: {avg_val_loss:.4f} - Val Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    # Step 8: Track time per epoch\n",
        "    epoch_end_time = time.time()\n",
        "    print(f\"Time taken for epoch {epoch + 1}: {epoch_end_time - epoch_start_time:.2f} seconds\")\n",
        "\n",
        "# Save the trained embeddings\n",
        "trained_embeddings = model.embedding.get_weights()[0]\n",
        "context_embeddings = model.context_embedding.get_weights()[0]\n",
        "np.save(\"trained_embeddings.npy\", trained_embeddings)\n",
        "np.save(\"context_embeddings.npy\", context_embeddings)\n",
        "print(\"Trained embeddings saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb7Wx_rzZOOm",
        "papermill": {
          "duration": 1.591393,
          "end_time": "2020-10-10T13:36:51.938622",
          "exception": false,
          "start_time": "2020-10-10T13:36:50.347229",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Word Embeddings Projector\n",
        "\n",
        "Follow these steps to visualize the learned word embeddings using TensorFlow's Embedding Projector:\n",
        "\n",
        "1. Extract the weights of the embedding layer from your trained model.\n",
        "2. Save the weights into two files:\n",
        "   - `vecs.tsv`: This file will store the actual vector representations of words.\n",
        "   - `meta.tsv`: This file will store the associated metadata (e.g., word labels) for visualization.\n",
        "3. Go to [TensorFlow Embedding Projector](http://projector.tensorflow.org/).\n",
        "4. Upload the `vecs.tsv` and `meta.tsv` files created in the previous step.\n",
        "5. Explore the visualizations provided by TensorFlow's Embedding Projector.\n",
        "<font color=#ffb578>\n",
        "6.Save the visualization of a word that best demonstrate the quality of your embeddings as an image and store it near the notebook.\n",
        "7. Compress the folder into a `.zip` file and submit it as part of your work.\n",
        "\n",
        "</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-10T13:36:54.675412Z",
          "iopub.status.busy": "2020-10-10T13:36:54.674309Z",
          "iopub.status.idle": "2020-10-10T13:36:56.018539Z",
          "shell.execute_reply": "2020-10-10T13:36:56.017942Z"
        },
        "id": "fGpXtNRS-V_u",
        "papermill": {
          "duration": 2.703171,
          "end_time": "2020-10-10T13:36:56.018656",
          "exception": false,
          "start_time": "2020-10-10T13:36:53.315485",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d419097c-8109-4db2-fedf-f154ff8f3987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings saved to embedding_projector/vecs.tsv\n",
            "Metadata saved to embedding_projector/meta.tsv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "output_dir = \"embedding_projector\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Step 1: Access the embedding layer from the trained model\n",
        "embedding_layer = model.embedding\n",
        "\n",
        "# Step 2: Extract the weights from the embedding layer\n",
        "embedding_weights = embedding_layer.get_weights()[0]  # Shape: (vocab_size, embedding_dim)\n",
        "\n",
        "# Step 3: Open files to store embeddings and metadata\n",
        "vecs_file = os.path.join(output_dir, \"vecs.tsv\")\n",
        "meta_file = os.path.join(output_dir, \"meta.tsv\")\n",
        "\n",
        "with open(vecs_file, 'w', encoding='utf-8') as vecs, open(meta_file, 'w', encoding='utf-8') as meta:\n",
        "    # Step 4: Iterate through the tokenizer's vocabulary\n",
        "    for word, idx in tokenizer.word_index.items():\n",
        "        if idx < vocab_size:  # Ensure index is within vocab_size\n",
        "            embedding_vector = embedding_weights[idx]\n",
        "            vecs.write('\\t'.join([str(x) for x in embedding_vector]) + '\\n')  # Write vector\n",
        "            meta.write(word + '\\n')  # Write word (metadata)\n",
        "\n",
        "# Step 5: Ensure the files are properly saved and closed\n",
        "print(f\"Embeddings saved to {vecs_file}\")\n",
        "print(f\"Metadata saved to {meta_file}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "papermill": {
      "duration": 1467.163823,
      "end_time": "2020-10-10T13:37:04.319726",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2020-10-10T13:12:37.155903",
      "version": "2.1.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}